---
name: Concepts
route: /overview/concepts
menu: Overview
---

# Concepts

Scrapers go through a series of steps to pull data from a website â”€ those generated using scrapegoat are no different.
These steps typically follow as such:

1. Generate a URL for the page to scrape
2. Fetch the page
3. Parse the page's HTML into a DOM
4. Extract data from the DOM

## Pages

A **page**, as scrapegoat sees it, is an object which describes how to perform each of those 4 steps.

Most of the time, steps 2-3 (fetching and parsing the HTML) will always be the same, so scrapegoat provides a `page` function, which will generate a page for you based on:

- how to generate the URL of the page, and
- how to extract data from the DOM.

A simple example of this might look like:

```ts
import { page, section, text, list } from 'scrapegoat'

type Args = { postId: number }
type BlogPost = { title: string, body: string, comments: Comment[] }
type Comment = { author: string, avatar: string, body: string }

export const blogPostPage = page<Args, BlogPost>({
  url: ({ postId }) => `https://mycool.blog/posts/${postId}`,
  scrape: section<BlogPost>('main', {
    title: text('header h1'),
    body: text('article'),
    comments: list<Comment>('.comments', section('.comment', {
      author: text('.author'),
      avatar: attr('.avatar', 'src', x => x)
      body: text('article')
    }))
  })
})
```

You can learn more about pages in the [tutorial](./tutorial/blog).

## Scrapers

Put simply, scrapers are pure functions that take a [Cheerio](https://cheerio.js.org/) DOM and return data extracted from it.
